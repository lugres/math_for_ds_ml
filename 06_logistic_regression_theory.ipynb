{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression and Classification\n",
    "* Logistic regression is trained on an output variable that is discrete (a binary 1 or 0) or a categorical number (which is a whole number). \n",
    "* It does output a continuous variable (from 0 to 1) in the form of probability, but that can be converted into a discrete value with a threshold.\n",
    "* it's an S-shaped curve (a sigmoid curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0/(1.0 + 16.8272567955368*exp(-0.62*x))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sympy.plotting.backends.matplotlibbackend.matplotlib.MatplotlibBackend at 0x108a8add0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "b0, b1, x = symbols('b0 b1 x')\n",
    "p=1.0/(1.0+exp(-(b0+b1*x)))\n",
    "\n",
    "p = p.subs(b0,-2.823) \n",
    "p = p.subs(b1, 0.620) \n",
    "print(p)\n",
    "\n",
    "plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the Logistic Curve\n",
    "* use maximum likelihood estimation (MLE), which, as the name suggests, maximizes the likelihood a given logistic curve would output the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69268939]\n",
      "[-3.17580504]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1]\n",
    "# Extract output column (all rows, last column)\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "# Perform logistic regression\n",
    "# Turn off penalty\n",
    "model = LogisticRegression(penalty=None)\n",
    "model.fit(X, Y) \n",
    "\n",
    "# print beta1\n",
    "print(model.coef_.flatten()) # 0.69267212 \n",
    "# print beta0\n",
    "print(model.intercept_.flatten()) # -3.17576395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.]), array([[0.37861083, 0.62138917]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the class (0 or 1), Predict the probability (of class 0 and 1)\n",
    "model.predict([[5.3]]), model.predict_proba([[5.3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Maximum Likelihood and Gradient Descent\n",
    "* Applying the idea of joint probabilities (multiplying them together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 1.0,\t y: 0,\t logistic function: 0.077052,        \t 1 - logit: 0.922948\n",
      "x: 1.5,\t y: 0,\t logistic function: 0.105575,        \t 1 - logit: 0.894425\n",
      "x: 2.1,\t y: 0,\t logistic function: 0.151723,        \t 1 - logit: 0.848277\n",
      "x: 2.4,\t y: 0,\t logistic function: 0.180443,        \t 1 - logit: 0.819557\n",
      "x: 2.5,\t y: 1,\t logistic function: 0.190914,        \t 1 - logit: 0.809086\n",
      "x: 3.1,\t y: 0,\t logistic function: 0.263379,        \t 1 - logit: 0.736621\n",
      "x: 4.2,\t y: 0,\t logistic function: 0.433756,        \t 1 - logit: 0.566244\n",
      "x: 4.4,\t y: 1,\t logistic function: 0.468042,        \t 1 - logit: 0.531958\n",
      "x: 4.6,\t y: 1,\t logistic function: 0.502632,        \t 1 - logit: 0.497368\n",
      "x: 4.9,\t y: 0,\t logistic function: 0.554367,        \t 1 - logit: 0.445633\n",
      "x: 5.2,\t y: 1,\t logistic function: 0.604949,        \t 1 - logit: 0.395051\n",
      "x: 5.6,\t y: 0,\t logistic function: 0.668897,        \t 1 - logit: 0.331103\n",
      "x: 6.1,\t y: 1,\t logistic function: 0.740686,        \t 1 - logit: 0.259314\n",
      "x: 6.4,\t y: 1,\t logistic function: 0.778567,        \t 1 - logit: 0.221433\n",
      "x: 6.6,\t y: 1,\t logistic function: 0.801528,        \t 1 - logit: 0.198472\n",
      "x: 7.0,\t y: 0,\t logistic function: 0.841968,        \t 1 - logit: 0.158032\n",
      "x: 7.6,\t y: 1,\t logistic function: 0.889785,        \t 1 - logit: 0.110215\n",
      "x: 7.8,\t y: 1,\t logistic function: 0.902655,        \t 1 - logit: 0.097345\n",
      "x: 8.4,\t y: 1,\t logistic function: 0.933559,        \t 1 - logit: 0.066441\n",
      "x: 8.8,\t y: 1,\t logistic function: 0.948814,        \t 1 - logit: 0.051186\n",
      "x: 9.2,\t y: 1,\t logistic function: 0.960714,        \t 1 - logit: 0.039286\n",
      "Joint likelihood: 0.000048\n"
     ]
    }
   ],
   "source": [
    "# Calculating the joint likelihood of observing all the points for a given logistic regression\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "patient_data = pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\").itertuples()\n",
    "\n",
    "b0 = -3.17576395\n",
    "b1 = 0.69267212\n",
    "\n",
    "def logistic_function(x):\n",
    "    p = 1.0 / (1.0 + math.exp(-(b0 + b1 * x)))\n",
    "    return p\n",
    "\n",
    "# Calculate the joint likelihood\n",
    "joint_likelihood = 1.0\n",
    "\n",
    "for p in patient_data: \n",
    "    # if p.y == 1.0:\n",
    "    #     joint_likelihood *= logistic_function(p.x) \n",
    "    # elif p.y == 0.0:\n",
    "    #     joint_likelihood *= (1.0 - logistic_function(p.x))\n",
    "    \n",
    "    # a shorter way:\n",
    "    joint_likelihood *= logistic_function(p.x) ** p.y * \\\n",
    "                        (1.0 - logistic_function(p.x)) ** (1.0 - p.y)\n",
    "    \n",
    "    print(f'x: {p.x},\\t y: {p.y},\\t logistic function: {logistic_function(p.x):4f},\\\n",
    "        \\t 1 - logit: {1 - logistic_function(p.x):4f}')\n",
    "\n",
    "print(f'Joint likelihood: {joint_likelihood:5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# use logarithmic addition instead of multiplication to fight floating point underflow\n",
    "joint_likelihood = 0.0\n",
    "\n",
    "for p in patient_data:\n",
    "    joint_likelihood += math.log(logistic_function(p.x) ** p.y * \\\n",
    "                                 (1.0 - logistic_function(p.x)) ** (1.0 - p.y))\n",
    "\n",
    "joint_likelihood = math.exp(joint_likelihood)\n",
    "print(joint_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6926693075370818 -3.175751550409824\n"
     ]
    }
   ],
   "source": [
    "# Using gradient descent on logistic regression\n",
    "from sympy import * \n",
    "import pandas as pd\n",
    "\n",
    "points = list(pd.read_csv(\"https://tinyurl.com/y2cocoo7\").itertuples())\n",
    "\n",
    "b1, b0, i, n = symbols('b1 b0 i n')\n",
    "x, y = symbols('x y', cls=Function)\n",
    "joint_likelihood = Sum(log((1.0 / (1.0 + exp(-(b0 + b1 * x(i))))) ** y(i) \\\n",
    "        * (1.0 - (1.0 / (1.0 + exp(-(b0 + b1 * x(i)))))) ** (1 - y(i))), (i, 0, n))\n",
    "\n",
    "# Partial derivative for m, with points substituted\n",
    "d_b1 = diff(joint_likelihood, b1) \\\n",
    "                .subs(n, len(points) - 1).doit() \\\n",
    "                .replace(x, lambda i: points[i].x) \\\n",
    "                .replace(y, lambda i: points[i].y)\n",
    "\n",
    "# Partial derivative for m, with points substituted\n",
    "d_b0 = diff(joint_likelihood, b0) \\\n",
    "                .subs(n, len(points) - 1).doit() \\\n",
    "                .replace(x, lambda i: points[i].x) \\\n",
    "                .replace(y, lambda i: points[i].y)\n",
    "\n",
    "# compile using lambdify for faster computation\n",
    "d_b1 = lambdify([b1, b0], d_b1)\n",
    "d_b0 = lambdify([b1, b0], d_b0)\n",
    "\n",
    "# Perform Gradient Descent\n",
    "b1 = 0.01 \n",
    "b0 = 0.01 \n",
    "L = .01\n",
    "\n",
    "for j in range(10_000):\n",
    "    b1 += d_b1(b1, b0) * L \n",
    "    b0 += d_b0(b1, b0) * L\n",
    "\n",
    "print(b1, b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariable Logistic Regression - exploring possibly discriminatory input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COEFFICIENTS: [ 0.03216406  0.03683014 -2.50414078  0.97423186]\n",
      "INTERCEPT: [-2.73503152]\n",
      "Sex: 1, age: 34, promotions: 1, years employed 5\n",
      "WILL LEAVE: [[0.28569689 0.71430311]]\n",
      "Sex: 1, age: 28, promotions: 2, years employed 3\n",
      "WILL STAY: [[0.97718392 0.02281608]]\n",
      "Sex: 1, age: 70, promotions: 8, years employed 22\n",
      "WILL LEAVE: [[0.21836545 0.78163455]]\n",
      "Sex: 1, age: 70, promotions: 8, years employed 21\n",
      "WILL LEAVE: [[0.4253166 0.5746834]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m (sex, age, promotions, years_employed) \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSex: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, age: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, promotions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpromotions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, years employed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myears_employed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_employee_will_stay(\u001b[38;5;28mint\u001b[39m(sex), \u001b[38;5;28mint\u001b[39m(age), \u001b[38;5;28mint\u001b[39m(promotions),\n\u001b[1;32m     39\u001b[0m       \u001b[38;5;28mint\u001b[39m(years_employed)))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "# Doing a multivariable logistic regression on employee data\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "employee_data = pd.read_csv(\"https://tinyurl.com/y6r7qjrp\") \n",
    "\n",
    "# grab independent variable columns\n",
    "inputs = employee_data.iloc[:, :-1]\n",
    "# grab dependent \"did_quit\" variable column\n",
    "output = employee_data.iloc[:, -1] \n",
    "\n",
    "# build logistic regression\n",
    "fit = LogisticRegression(penalty=None).fit(inputs, output)\n",
    "\n",
    "# Print coefficients:\n",
    "print(\"COEFFICIENTS: {0}\".format(fit.coef_.flatten())) \n",
    "print(\"INTERCEPT: {0}\".format(fit.intercept_.flatten()))\n",
    "\n",
    "# Interact and test with new employee data\n",
    "def predict_employee_will_stay(sex, age, promotions, years_employed):\n",
    "    pred_df = pd.DataFrame([[sex, age, promotions, years_employed]], \n",
    "                           columns=['SEX', 'AGE', 'PROMOTIONS', 'YEARS_EMPLOYED']) \n",
    "    prediction = fit.predict(pred_df) \n",
    "    probabilities = fit.predict_proba(pred_df) \n",
    "    if prediction == [[1]]:\n",
    "        return \"WILL LEAVE: {0}\".format(probabilities) \n",
    "    else:\n",
    "        return \"WILL STAY: {0}\".format(probabilities)\n",
    "\n",
    "# Test a prediction\n",
    "while True:\n",
    "    n = input(\"Predict employee will stay or leave {sex},\\\n",
    "        {age},{promotions},{years employed}: \")\n",
    "    if n is ('q' or 'Q'):\n",
    "        break\n",
    "    (sex, age, promotions, years_employed) = n.split(\",\") \n",
    "    print(f'Sex: {sex}, age: {age}, promotions: {promotions}, years employed {years_employed}')\n",
    "    print(predict_employee_will_stay(int(sex), int(age), int(promotions),\n",
    "          int(years_employed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WILL LEAVE: [[0.43025683 0.56974317]]'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_employee_will_stay(1,43,8,22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This employee retention analysis data has been created with the idea behind that an employee will quit if not promoted every two years.\n",
    "* model will fall apart on data it has not being trained on (e.g. 70 y.o.)\n",
    "* sex and age params are given low weights, meaning the model won't properly assess employees who have been at a company for many years and has never been promoted - the ones who are loyal and happy, not leaving anytime soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful with data privacy concerns, possible legal and PR issues while making classification on People! \n",
    "* especially if models are discriminatory, and undesirable outcomes are inflicted (not being hired, denied loans)\n",
    "\n",
    "### Analyse data sources carefully, be curious! \n",
    "* where data comes from, what assumptions are built into it - what process created data, what time peiod was used, how we make sure there are no false negatives (empl are about to quit)\n",
    "* what predictions are going to be used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Log-Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* β0 + β1x - the log-odds function\n",
    "* it is equal to logit = log (p / (1 − p))\n",
    "* it is easier to compare one set of odds against another. We treat anything greater than 0 as favoring odds an event will happen, whereas anything less than 0 is against an event.\n",
    "* we can compare the effect between one x-value and another, e.g. at 6 and 8. Getting a value of an odds ratio 3.996, meaning that our odds of showing symptoms increases by nearly a factor of four with an extra two hours of exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Squared\n",
    "* indicates how well a given independent variable explains a dependent variable\n",
    "* we can have McFadden’s Pseudo R2\n",
    " * R2 = log likelihood − log likelihood fit \\ log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'map' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m log_likelihood_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(log(logistic_function(p\u001b[38;5;241m.\u001b[39mx)) \u001b[38;5;241m*\u001b[39m p\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      2\u001b[0m                         log(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m logistic_function(p\u001b[38;5;241m.\u001b[39mx)) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m p\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m      3\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m patient_data)\n\u001b[0;32m----> 5\u001b[0m likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39my \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m patient_data) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpatient_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(log(likelihood)\u001b[38;5;241m*\u001b[39mp\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m+\u001b[39m log(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m likelihood)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m p\u001b[38;5;241m.\u001b[39my) \\\n\u001b[1;32m      7\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m patient_data)\n\u001b[1;32m      9\u001b[0m r2 \u001b[38;5;241m=\u001b[39m (log_likelihood \u001b[38;5;241m-\u001b[39m log_likelihood_fit) \u001b[38;5;241m/\u001b[39m log_likelihood\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'map' has no len()"
     ]
    }
   ],
   "source": [
    "log_likelihood_fit = sum(log(logistic_function(p.x)) * p.y +\n",
    "                        log(1.0 - logistic_function(p.x)) * (1.0 - p.y)\n",
    "                        for p in patient_data)\n",
    "\n",
    "likelihood = sum(p.y for p in patient_data) / len(patient_data)\n",
    "log_likelihood = sum(log(likelihood)*p.y + log(1.0 - likelihood)*(1.0 - p.y) \\\n",
    "                    for p in patient_data)\n",
    "\n",
    "r2 = (log_likelihood - log_likelihood_fit) / log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.306456105756576\n"
     ]
    }
   ],
   "source": [
    "# Calculating the R2 for a logistic regression import pandas as pd\n",
    "\n",
    "from math import log, exp\n",
    "patient_data = list(pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\") \\\n",
    "                                .itertuples())\n",
    "\n",
    "# Declare fitted logistic regression\n",
    "b0 = -3.17576395\n",
    "b1 = 0.69267212\n",
    "\n",
    "def logistic_function(x): \n",
    "    p=1.0/(1.0+exp(-(b0+b1*x))) \n",
    "    return p\n",
    "\n",
    "# calculate the log likelihood of the fit\n",
    "log_likelihood_fit = sum(log(logistic_function(p.x)) * p.y +\n",
    "                         log(1.0 - logistic_function(p.x)) * (1.0 - p.y)\n",
    "                        for p in patient_data) \n",
    "\n",
    "# calculate the log likelihood without fit\n",
    "likelihood = sum(p.y for p in patient_data) / len(patient_data)\n",
    "\n",
    "log_likelihood = sum(log(likelihood) * p.y + log(1.0 - likelihood) * (1.0 - p.y) \\\n",
    "                    for p in patient_data) \n",
    "\n",
    "# calculate R-Square\n",
    "r2 = (log_likelihood - log_likelihood_fit) / log_likelihood \n",
    "\n",
    "print(r2) # 0.306456105756576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can conclude that hours of exposure is mediocre for predicting symptoms, as the R2 is 0.30645 which pretty far from 1. \\\\\n",
    "\n",
    " There must be variables other than time exposure that better predict if someone will show symptoms. This makes sense because we have a large mix of patients showing symptoms versus not showing symptoms for most of our observed data (no clean division b/w True and False cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-value \n",
    "\n",
    "* We'll use chi-square (pronounced kai) distribution, annotated as χ2 distribution.\n",
    "* the degrees of freedom will depend on how many parameters n are in our logistic regression, which will be n − 1.\n",
    "* p‐value = chi (2 ((log likelihood fit) − (log likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016604875618753787\n"
     ]
    }
   ],
   "source": [
    "# Calculating a p-value for a given logistic regression\n",
    "    \n",
    "import pandas as pd\n",
    "from math import log, exp \n",
    "from scipy.stats import chi2\n",
    "\n",
    "patient_data = list(pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\").itertuples())\n",
    "\n",
    "# Declare fitted logistic regression\n",
    "b0 = -3.17576395\n",
    "b1 = 0.69267212\n",
    "\n",
    "def logistic_function(x): \n",
    "    p=1.0/(1.0+exp(-(b0+b1*x))) \n",
    "    return p\n",
    "\n",
    "# calculate the log likelihood of the fit\n",
    "log_likelihood_fit = sum(log(logistic_function(p.x)) * p.y +\n",
    "                         log(1.0 - logistic_function(p.x)) * (1.0 - p.y)\n",
    "                        for p in patient_data) \n",
    "\n",
    "# calculate the log likelihood without fit\n",
    "likelihood = sum(p.y for p in patient_data) / len(patient_data)\n",
    "log_likelihood = sum(log(likelihood) * p.y + log(1.0 - likelihood) * (1.0 - p.y) \\\n",
    "                    for p in patient_data)\n",
    "\n",
    "# calculate p-value\n",
    "chi2_input = 2 * (log_likelihood_fit - log_likelihood)\n",
    "p_value = chi2.pdf(chi2_input, 1) # 1 degree of freedom (n - 1)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold for signifiance is .05, we say this data is statistically significant and was not by random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Splits\n",
    "\n",
    "* we can use train/test splits as a way to validate machine learning algorithms. \n",
    "* This is the more machine learning approach to assessing the performance of a logistic regression. It is hard to rely on traditional statistical metrics like R2 and p-values when you are dealing with many variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Mean: 0.611 (stdev=0.000)\n"
     ]
    }
   ],
   "source": [
    "# Performing a logistic regression with three-fold cross-validation\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"https://tinyurl.com/y6r7qjrp\", delimiter=\",\")\n",
    "\n",
    "X = df.values[:, :-1]\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "# \"random_state\" is the random seed, which we fix to 7\n",
    "kfold = KFold(n_splits=3, random_state=7, shuffle=True)\n",
    "model = LogisticRegression(penalty=None)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy Mean: %.3f (stdev=%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "* accuracy by itself is not a good performance measure for classification model. It could be horrendously misleading, especially for imbalanced data where the event of interest (e.g., a quitting employee) is rare.\n",
    "* Therefore we have to assess confusion matrix - a grid that breaks out the predictions against the actual out‐ comes showing the true positives, true negatives, false positives (type I error), and false negatives (type II error)\n",
    "* we want the diagonal values (top-left to bottom-right) to be higher because these reflect correct classifications (true positives - employees who were predicted to quit actually did quit, true negatives - employees who were predicted to stay actually did stay )\n",
    "* useful metrics can be derived from a confusion matrix, such as precision (how accurate positive predictions were, TP/(TP+FP)) and sensitivity/recall (rate of identified positives, TP/(TP+FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 3]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "# Creating a confusion matrix for a testing dataset\n",
    "    \n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('https://bit.ly/3cManTi', delimiter=\",\")\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = df.values[:, :-1]\n",
    "\n",
    "# Extract output column (all rows, last column)\\\n",
    "Y = df.values[:, -1]\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.33,\n",
    "    random_state=10)\n",
    "model.fit(X_train, Y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "\"\"\"\"\n",
    "[[truepositives falsenegatives]\n",
    " [falsepositives truenegatives]]\n",
    "\n",
    "The diagonal represents correct predictions,\n",
    "so we want those to be higher\n",
    "\"\"\"\n",
    "\n",
    "matrix = confusion_matrix(y_true=Y_test, y_pred=prediction) \n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristics (ROC) / Area Under Curve (AUC) metrics\n",
    "\n",
    "* can summarize all of confusion matrices with a receiver operator characteristic (ROC) curve\n",
    "* True Positive Rate (Sensinivity) - y, False Positive Rate (1 - Specificity) - x.\n",
    "* allows us to see each testing instance (each represented by a black dot) and find an agreeable balance between true positives and false positives.\n",
    "* can be used to compare different ML configurations and models, by comparing two models by their area under the curve (AUC) with their respective ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.791 (0.051)\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='roc_auc') \n",
    "print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "# AUC: 0.791 (0.051)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "\n",
    "* happens when data is not equally represented across every outcome class\n",
    "* many problems of interest are imbalanced, such as disease prediction, security breaches, fraud detection, and so on. \n",
    "* Class imbalance is still an open problem with no great solution, but there're some techniques to use: \n",
    "* * collect more data or try different models as well as use confusion matrices and ROC/AUC curves.\n",
    "* * duplicate samples in the minority class until it is equally represented in the dataset (stratify option specifying the column)\n",
    "* * generate synthetic samples of the minority class with a family of algorithms called SMOTE.\n",
    "* * tackle the problem in a way that uses anomaly-detection models, which are deliberately designed for seeking out a rare event."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
